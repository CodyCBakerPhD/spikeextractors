{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curation\n",
    "\n",
    "In this tutorial, we will go over what CurationSortingExtractors are and how they can be used to curate the results in a SortingExtractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spikeextractors as se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define the properties of the in-memory dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels=7\n",
    "samplerate=30000\n",
    "duration=20\n",
    "num_timepoints=int(samplerate*duration)\n",
    "num_units=5\n",
    "num_events=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a pure-noise timeseries dataset recorded by a linear probe geometry, generate some random events, and then define in-memory sorting and recording extractors.\n",
    "\n",
    "We will add some spike features to the units to show how splitting and merging units effect spike features. Any spike features that are shared across *all* units will be split and merged correctly, otherwise the features that are not shared across split and merged units will be removed from the CurationSortingExtractor.\n",
    "\n",
    "Unit properties will automatically be removed from the CurationSortingExtractor when splitting and mergin as they are not well-defined for those operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a pure-noise timeseries dataset and a linear geometry\n",
    "np.random.seed(0)\n",
    "timeseries=np.random.normal(0,10,(num_channels,num_timepoints))\n",
    "geom=np.zeros((num_channels,2))\n",
    "geom[:,0]=range(num_channels)\n",
    "\n",
    "# Define the in-memory recording extractor\n",
    "RX=se.NumpyRecordingExtractor(timeseries=timeseries,geom=geom,samplerate=samplerate)\n",
    "\n",
    "# Generate some random events\n",
    "times=np.int_(np.sort(np.random.uniform(0,num_timepoints,num_events)))\n",
    "labels=np.random.randint(1,num_units+1,size=num_events)\n",
    "    \n",
    "# Define the in-memory sorting extractor\n",
    "SX=se.NumpySortingExtractor()\n",
    "for k in range(1,num_units+1):\n",
    "    times_k=times[np.where(labels==k)[0]]\n",
    "    SX.add_unit(unit_id=k,times=times_k)\n",
    "    \n",
    "#Add some features to the sorting extractor. These will be merged and split appropriately during curation\n",
    "spikes = 0\n",
    "for unit_id in SX.get_unit_ids():\n",
    "    SX.set_unit_spike_features(unit_id, feature_name='f_int', value=range(spikes, spikes + len(SX.get_unit_spike_train(unit_id))))\n",
    "    spikes += len(SX.get_unit_spike_train(unit_id))\n",
    "    \n",
    "spikes = 0\n",
    "for unit_id in SX.get_unit_ids():\n",
    "    SX.set_unit_spike_features(unit_id, feature_name='f_float', value=np.arange(float(spikes) + .1, float(spikes + len(SX.get_unit_spike_train(unit_id) + .1))))\n",
    "    spikes += len(SX.get_unit_spike_train(unit_id))\n",
    "    \n",
    "#Features that are not shared across ALL units will not be merged and split correctly (will disappear)\n",
    "SX.set_unit_spike_features(1, feature_name='bad_feature', value=np.repeat(1, len(SX.get_unit_spike_train(1))))\n",
    "SX.set_unit_spike_features(2, feature_name='bad_feature', value=np.repeat(2, len(SX.get_unit_spike_train(2))))\n",
    "SX.set_unit_spike_features(3, feature_name='bad_feature', value=np.repeat(3, len(SX.get_unit_spike_train(3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unit ids = {}'.format(SX.get_unit_ids()))\n",
    "st=SX.get_unit_spike_train(unit_id=1)\n",
    "print('Num. events for unit 1 = {}'.format(len(st)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can curate the results using a CurationSortingExtractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSX = se.CurationSortingExtractor(parent_sorting=SX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Curated Unit Ids: \" + str(CSX.get_unit_ids()))\n",
    "print(\"Original Unit Ids: \" + str(SX.get_unit_ids()))\n",
    "\n",
    "print(\"Curated ST: \" + str(CSX.get_unit_spike_train(1)))\n",
    "print(\"Original ST: \" + str(SX.get_unit_spike_train(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets split one unit from the sorting result (this could be two units incorrectly clustered as one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSX.split_unit(unit_id=1, indices=[0, 1])\n",
    "print(\"Curated Unit Ids: \" + str(CSX.get_unit_ids()))\n",
    "print(\"Original Spike Train: \" + str(SX.get_unit_spike_train(1)))\n",
    "print(\"Split Spike Train 1: \" + str(CSX.get_unit_spike_train(6)))\n",
    "print(\"Split Spike Train 2: \" + str(CSX.get_unit_spike_train(7)))\n",
    "for unit_id in CSX.get_unit_ids():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the split was incorrect, we can always merge the two units back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSX.merge_units(unit_ids=[6, 7])\n",
    "print(\"Curated Spike Train: \" + str(CSX.get_unit_spike_train(8)))\n",
    "print(\"Original Spike Train: \" + str(SX.get_unit_spike_train(1)))\n",
    "for unit_id in CSX.get_unit_ids():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also exclude units, so let's get rid of 8 since we are seem to be confused about this unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSX.exclude_units(unit_ids=[8])\n",
    "for unit_id in CSX.get_unit_ids():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's merge 3 and 4 together (This will create a new unit which encapsulates both previous units)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSX.merge_units(unit_ids=[3, 4])\n",
    "print(\"Curated Unit Ids: \" + str(CSX.get_unit_ids()))\n",
    "print(\"Merged Spike Train: \" + str(CSX.get_unit_spike_train(9)))\n",
    "print(\"Original Spike Trains concatenated: \" + str(np.sort(np.concatenate((SX.get_unit_spike_train(3), SX.get_unit_spike_train(4))))))\n",
    "print(\"\\nCuration Tree\")\n",
    "for unit_id in CSX.get_unit_ids():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's merge units 2 and 6 together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSX.merge_units(unit_ids=[2, 9])\n",
    "print(\"Curated Unit Ids: \" + str(CSX.get_unit_ids()))\n",
    "print(\"Merged Spike Train: \" + str(CSX.get_unit_spike_train(10)))\n",
    "merged_spike_train = []\n",
    "for unit_id in SX.get_unit_ids():\n",
    "    if(unit_id != 1 and unit_id != 5):\n",
    "        merged_spike_train.append(SX.get_unit_spike_train(unit_id))\n",
    "merged_spike_train = np.asarray(merged_spike_train)\n",
    "merged_spike_train = np.sort(np.concatenate(merged_spike_train).ravel())\n",
    "print(\"Original Spike Trains concatenated: \" + str(merged_spike_train))\n",
    "print(\"\\nCuration Tree\")\n",
    "for unit_id in CSX.get_unit_ids():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split unit 5 with given indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSX.split_unit(unit_id=5, indices=[0, 1])\n",
    "print(\"Curated Unit Ids: \" + str(CSX.get_unit_ids()))\n",
    "print(\"Original Spike Train: \" + str(SX.get_unit_spike_train(5)))\n",
    "print(\"Split Spike Train 1: \" + str(CSX.get_unit_spike_train(11)))\n",
    "print(\"Split Spike Train 2: \" + str(CSX.get_unit_spike_train(12)))\n",
    "print(\"\\nCuration Tree\")\n",
    "for unit_id in CSX.get_unit_ids():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can merge units 10 and 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSX.merge_units(unit_ids=[10, 11])\n",
    "print(\"Curated Unit Ids: \" + str(CSX.get_unit_ids()))\n",
    "print(\"Merged Spike Train: \" + str(CSX.get_unit_spike_train(13)))\n",
    "original_spike_train = (np.sort(np.concatenate((SX.get_unit_spike_train(3), SX.get_unit_spike_train(4), SX.get_unit_spike_train(2), SX.get_unit_spike_train(5)[np.asarray([0,1])]))))\n",
    "print(\"Original Spike Train: \" + str(original_spike_train))\n",
    "print(\"\\nCuration Tree\")\n",
    "for unit_id in CSX.get_unit_ids():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now write the input/output in the MountainSort format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se.MdaRecordingExtractor.write_recording(recording=RX,save_path='sample_mountainsort_dataset')\n",
    "se.MdaSortingExtractor.write_sorting(sorting=CSX,save_path='sample_mountainsort_dataset/firings_true.mda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read this dataset with the Mda input extractor (we can now have a normal sorting extractor with our curations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RX2=se.MdaRecordingExtractor(dataset_directory='sample_mountainsort_dataset')\n",
    "SX2=se.MdaSortingExtractor(firings_file='sample_mountainsort_dataset/firings_true.mda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"New Unit Ids: \" + str(SX2.get_unit_ids()))\n",
    "print(\"New Unit Spike Train: \" + str(SX2.get_unit_spike_train(13)))\n",
    "print(\"Previous Curated Unit Spike Train: \" + str(CSX.get_unit_spike_train(13)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
