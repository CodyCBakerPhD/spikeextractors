{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import spikeextractors as se\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properties of the in-memory dataset\n",
    "num_channels=7\n",
    "samplerate=30000\n",
    "duration=20\n",
    "num_timepoints=int(samplerate*duration)\n",
    "num_units=5\n",
    "num_events=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a pure-noise timeseries dataset and a linear geometry\n",
    "timeseries=np.random.normal(0,10,(num_channels,num_timepoints))\n",
    "geom=np.zeros((num_channels,2))\n",
    "geom[:,0]=range(num_channels)\n",
    "\n",
    "# Define the in-memory recording extractor\n",
    "RX=se.NumpyRecordingExtractor(timeseries=timeseries,geom=geom,samplerate=samplerate)\n",
    "\n",
    "# Generate some random events\n",
    "times=np.int_(np.sort(np.random.uniform(0,num_timepoints,num_events)))\n",
    "labels=np.random.randint(1,num_units+1,size=num_events)\n",
    "    \n",
    "# Define the in-memory sorting extractor\n",
    "SX=se.NumpySortingExtractor()\n",
    "for k in range(1,num_units+1):\n",
    "    times_k=times[np.where(labels==k)[0]]\n",
    "    SX.add_unit(unit_id=k,times=times_k)\n",
    "    \n",
    "#Add some features to the sorting extractor. These will be merged and split appropriately during curation\n",
    "spikes = 0\n",
    "for unit_id in SX.get_unit_ids():\n",
    "    SX.set_unit_spike_features(unit_id, feature_name='f_int', value=range(spikes, spikes + len(SX.get_unit_spike_train(unit_id))))\n",
    "    spikes += len(SX.get_unit_spike_train(unit_id))\n",
    "    \n",
    "spikes = 0\n",
    "for unit_id in SX.get_unit_ids():\n",
    "    SX.set_unit_spike_features(unit_id, feature_name='f_float', value=np.arange(float(spikes) + .1, float(spikes + len(SX.get_unit_spike_train(unit_id) + .1))))\n",
    "    spikes += len(SX.get_unit_spike_train(unit_id))\n",
    "    \n",
    "#Features that are not shared across ALL units will not be merged and split correctly (will disappear)\n",
    "SX.set_unit_spike_features(1, feature_name='bad_feature', value=np.repeat(1, len(SX.get_unit_spike_train(1))))\n",
    "SX.set_unit_spike_features(2, feature_name='bad_feature', value=np.repeat(2, len(SX.get_unit_spike_train(2))))\n",
    "SX.set_unit_spike_features(3, feature_name='bad_feature', value=np.repeat(3, len(SX.get_unit_spike_train(3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the API for extracting information\n",
    "print('Unit ids = {}'.format(SX.get_unit_ids()))\n",
    "st=SX.get_unit_spike_train(unit_id=1)\n",
    "print('Num. events for unit 1 = {}'.format(len(st)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can curate the results using a CuratedSortingExtractor\n",
    "CSX = se.CuratedSortingExtractor(parent_sorting=SX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Curated Unit Ids: \" + str(CSX.get_unit_ids()))\n",
    "print(\"Original Unit Ids: \" + str(SX.get_unit_ids()))\n",
    "\n",
    "print(\"Curated ST: \" + str(CSX.get_unit_spike_train(1)))\n",
    "print(\"Original ST: \" + str(SX.get_unit_spike_train(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets split one unit from the sorting result (this could be two units incorrectly clustered as one)\n",
    "CSX.split_unit(unit_id=1, indices=[0, 1])\n",
    "print(\"Curated Unit Ids: \" + str(CSX.get_unit_ids()))\n",
    "print(\"Original Spike Train: \" + str(SX.get_unit_spike_train(1)))\n",
    "print(\"Split Spike Train 1: \" + str(CSX.get_unit_spike_train(6)))\n",
    "print(\"Split Spike Train 2: \" + str(CSX.get_unit_spike_train(7)))\n",
    "for unit_id in CSX.get_unit_ids():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the split was incorrect, we can always merge the two units back together\n",
    "CSX.merge_units(unit_ids=[6, 7])\n",
    "print(\"Curated Spike Train: \" + str(CSX.get_unit_spike_train(8)))\n",
    "print(\"Original Spike Train: \" + str(SX.get_unit_spike_train(1)))\n",
    "for unit_id in CSX.get_unit_ids():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also exclude units, so let's get rid of 8 since we are just confused about this unit\n",
    "CSX.exclude_units(unit_ids=[8])\n",
    "for unit_id in CSX.get_unit_ids():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's merge 3 and 4 together (This will create a new unit which encapsulates both previous units)\n",
    "CSX.merge_units(unit_ids=[3, 4])\n",
    "print(\"Curated Unit Ids: \" + str(CSX.get_unit_ids()))\n",
    "print(\"Merged Spike Train: \" + str(CSX.get_unit_spike_train(9)))\n",
    "print(\"Original Spike Trains concatenated: \" + str(np.sort(np.concatenate((SX.get_unit_spike_train(3), SX.get_unit_spike_train(4))))))\n",
    "print(\"\\nCuration Tree\")\n",
    "for unit_id in CSX.get_unit_ids():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's merge 2 and 6 together\n",
    "\n",
    "CSX.merge_units(unit_ids=[2, 9])\n",
    "print(\"Curated Unit Ids: \" + str(CSX.get_unit_ids()))\n",
    "print(\"Merged Spike Train: \" + str(CSX.get_unit_spike_train(10)))\n",
    "merged_spike_train = []\n",
    "for unit_id in SX.get_unit_ids():\n",
    "    if(unit_id != 1 and unit_id != 5):\n",
    "        merged_spike_train.append(SX.get_unit_spike_train(unit_id))\n",
    "merged_spike_train = np.asarray(merged_spike_train)\n",
    "merged_spike_train = np.sort(np.concatenate(merged_spike_train).ravel())\n",
    "print(\"Original Spike Trains concatenated: \" + str(merged_spike_train))\n",
    "print(\"\\nCuration Tree\")\n",
    "for unit_id in CSX.get_unit_ids():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's split unit 5 with given indices\n",
    "\n",
    "CSX.split_unit(unit_id=5, indices=[0, 1])\n",
    "print(\"Curated Unit Ids: \" + str(CSX.get_unit_ids()))\n",
    "print(\"Original Spike Train: \" + str(SX.get_unit_spike_train(5)))\n",
    "print(\"Split Spike Train 1: \" + str(CSX.get_unit_spike_train(11)))\n",
    "print(\"Split Spike Train 2: \" + str(CSX.get_unit_spike_train(12)))\n",
    "print(\"\\nCuration Tree\")\n",
    "for unit_id in CSX.get_unit_ids():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we can merge units 7 and 8\n",
    "\n",
    "CSX.merge_units(unit_ids=[10, 11])\n",
    "print(\"Curated Unit Ids: \" + str(CSX.get_unit_ids()))\n",
    "print(\"Merged Spike Train: \" + str(CSX.get_unit_spike_train(13)))\n",
    "original_spike_train = (np.sort(np.concatenate((SX.get_unit_spike_train(3), SX.get_unit_spike_train(4), SX.get_unit_spike_train(2), SX.get_unit_spike_train(5)[np.asarray([0,1])]))))\n",
    "print(\"Original Spike Train: \" + str(original_spike_train))\n",
    "print(\"\\nCuration Tree\")\n",
    "for unit_id in CSX.get_unit_ids():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the input/output in the MountainSort format\n",
    "se.MdaRecordingExtractor.write_recording(recording=RX,save_path='sample_mountainsort_dataset')\n",
    "se.MdaSortingExtractor.write_sorting(sorting=CSX,save_path='sample_mountainsort_dataset/firings_true.mda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read this dataset with the Mda input extractor (we can now have a normal sorting extractor with our curations)\n",
    "RX2=se.MdaRecordingExtractor(dataset_directory='sample_mountainsort_dataset')\n",
    "SX2=se.MdaSortingExtractor(firings_file='sample_mountainsort_dataset/firings_true.mda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"New Unit Ids: \" + str(SX2.get_unit_ids()))\n",
    "print(\"New Unit Spike Train: \" + str(SX2.get_unit_spike_train(13)))\n",
    "print(\"Previous Curated Unit Spike Train: \" + str(CSX.get_unit_spike_train(13)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Current spike feature names\n",
    "print(CSX.get_unit_spike_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#All features have been appropriately merged and split according to previous operations\n",
    "print(CSX.get_unit_spike_features(12, 'f_int'))\n",
    "print(CSX.get_unit_spike_features(12, 'f_float'))\n",
    "print(CSX.get_unit_spike_features(13, 'f_int'))\n",
    "print(CSX.get_unit_spike_features(13, 'f_float'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
