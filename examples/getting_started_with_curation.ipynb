{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import spikeextractors as se\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properties of the in-memory dataset\n",
    "num_channels=7\n",
    "samplerate=30000\n",
    "duration=20\n",
    "num_timepoints=int(samplerate*duration)\n",
    "num_units=5\n",
    "num_events=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a pure-noise timeseries dataset and a linear geometry\n",
    "timeseries=np.random.normal(0,10,(num_channels,num_timepoints))\n",
    "geom=np.zeros((num_channels,2))\n",
    "geom[:,0]=range(num_channels)\n",
    "\n",
    "# Define the in-memory recording extractor\n",
    "RX=se.NumpyRecordingExtractor(timeseries=timeseries,geom=geom,samplerate=samplerate)\n",
    "\n",
    "# Generate some random events\n",
    "times=np.int_(np.sort(np.random.uniform(0,num_timepoints,num_events)))\n",
    "labels=np.random.randint(1,num_units+1,size=num_events)\n",
    "    \n",
    "# Define the in-memory sorting extractor\n",
    "SX=se.NumpySortingExtractor()\n",
    "for k in range(1,num_units+1):\n",
    "    times_k=times[np.where(labels==k)[0]]\n",
    "    SX.addUnit(unit_id=k,times=times_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the API for extracting information\n",
    "print('Unit ids = {}'.format(SX.getUnitIds()))\n",
    "st=SX.getUnitSpikeTrain(unit_id=1)\n",
    "print('Num. events for unit 1 = {}'.format(len(st)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can curate the results using a CuratedSortingExtractor\n",
    "\n",
    "CSX = se.CuratedSortingExtractor(parent_sorting=SX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Curated Unit Ids: \" + str(CSX.getUnitIds()))\n",
    "print(\"Original Unit Ids: \" + str(SX.getUnitIds()))\n",
    "\n",
    "print(\"Curated ST: \" + str(CSX.getUnitSpikeTrain(1)))\n",
    "print(\"Original ST: \" + str(SX.getUnitSpikeTrain(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets split one unit from the sorting result (this could be two units incorrectly clustered as one)\n",
    "\n",
    "CSX.splitUnit(unit_id=1, indices=[0, 1])\n",
    "print(\"Curated Unit Ids: \" + str(CSX.getUnitIds()))\n",
    "print(\"Original Spike Train: \" + str(SX.getUnitSpikeTrain(1)))\n",
    "print(\"Split Spike Train 1: \" + str(CSX.getUnitSpikeTrain(6)))\n",
    "print(\"Split Spike Train 2: \" + str(CSX.getUnitSpikeTrain(7)))\n",
    "for unit_id in CSX.getUnitIds():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the split was incorrect, we can always merge the two units back together\n",
    "CSX.mergeUnits(unit_ids=[6, 7])\n",
    "print(\"Curated Spike Train: \" + str(CSX.getUnitSpikeTrain(8)))\n",
    "print(\"Original Spike Train: \" + str(SX.getUnitSpikeTrain(1)))\n",
    "for unit_id in CSX.getUnitIds():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also exclude units, so let's get rid of 8 since we are just confused about this unit\n",
    "CSX.excludeUnits(unit_ids=[8])\n",
    "for unit_id in CSX.getUnitIds():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's merge 3 and 4 together (This will create a new unit which encapsulates both previous units)\n",
    "CSX.mergeUnits(unit_ids=[3, 4])\n",
    "print(\"Curated Unit Ids: \" + str(CSX.getUnitIds()))\n",
    "print(\"Merged Spike Train: \" + str(CSX.getUnitSpikeTrain(9)))\n",
    "print(\"Original Spike Trains concatenated: \" + str(np.sort(np.concatenate((SX.getUnitSpikeTrain(3), SX.getUnitSpikeTrain(4))))))\n",
    "print(\"\\nCuration Tree\")\n",
    "for unit_id in CSX.getUnitIds():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's merge 2 and 6 together\n",
    "\n",
    "CSX.mergeUnits(unit_ids=[2, 9])\n",
    "print(\"Curated Unit Ids: \" + str(CSX.getUnitIds()))\n",
    "print(\"Merged Spike Train: \" + str(CSX.getUnitSpikeTrain(10)))\n",
    "merged_spike_train = []\n",
    "for unit_id in SX.getUnitIds():\n",
    "    if(unit_id != 1 and unit_id != 5):\n",
    "        merged_spike_train.append(SX.getUnitSpikeTrain(unit_id))\n",
    "merged_spike_train = np.asarray(merged_spike_train)\n",
    "merged_spike_train = np.sort(np.concatenate(merged_spike_train).ravel())\n",
    "print(\"Original Spike Trains concatenated: \" + str(merged_spike_train))\n",
    "print(\"\\nCuration Tree\")\n",
    "for unit_id in CSX.getUnitIds():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's split unit 5 with given indices\n",
    "\n",
    "CSX.splitUnit(unit_id=5, indices=[0, 1])\n",
    "print(\"Curated Unit Ids: \" + str(CSX.getUnitIds()))\n",
    "print(\"Original Spike Train: \" + str(SX.getUnitSpikeTrain(5)))\n",
    "print(\"Split Spike Train 1: \" + str(CSX.getUnitSpikeTrain(11)))\n",
    "print(\"Split Spike Train 2: \" + str(CSX.getUnitSpikeTrain(12)))\n",
    "print(\"\\nCuration Tree\")\n",
    "for unit_id in CSX.getUnitIds():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we can merge units 7 and 8\n",
    "\n",
    "CSX.mergeUnits(unit_ids=[10, 11])\n",
    "print(\"Curated Unit Ids: \" + str(CSX.getUnitIds()))\n",
    "print(\"Merged Spike Train: \" + str(CSX.getUnitSpikeTrain(13)))\n",
    "original_spike_train = (np.sort(np.concatenate((SX.getUnitSpikeTrain(3), SX.getUnitSpikeTrain(4), SX.getUnitSpikeTrain(2), SX.getUnitSpikeTrain(5)[np.asarray([0,1])]))))\n",
    "print(\"Original Spike Train: \" + str(original_spike_train))\n",
    "print(\"\\nCuration Tree\")\n",
    "for unit_id in CSX.getUnitIds():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the input/output in the MountainSort format\n",
    "se.MdaRecordingExtractor.writeRecording(recording=RX,save_path='sample_mountainsort_dataset')\n",
    "se.MdaSortingExtractor.writeSorting(sorting=CSX,save_path='sample_mountainsort_dataset/firings_true.mda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read this dataset with the Mda input extractor (we can now have a normal sorting extractor with our curations)\n",
    "RX2=se.MdaRecordingExtractor(dataset_directory='sample_mountainsort_dataset')\n",
    "SX2=se.MdaSortingExtractor(firings_file='sample_mountainsort_dataset/firings_true.mda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"New Unit Ids: \" + str(SX2.getUnitIds()))\n",
    "print(\"New Unit Spike Train: \" + str(SX2.getUnitSpikeTrain(13)))\n",
    "print(\"Previous Curated Unit Spike Train: \" + str(CSX.getUnitSpikeTrain(13)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./recipy-cmd search test.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spikeinterface]",
   "language": "python",
   "name": "conda-env-spikeinterface-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
