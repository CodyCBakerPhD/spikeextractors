{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/colehurwitz/miniconda3/envs/spikeinterface/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/Users/colehurwitz/miniconda3/envs/spikeinterface/lib/python3.7/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/Users/colehurwitz/miniconda3/envs/spikeinterface/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/Users/colehurwitz/miniconda3/envs/spikeinterface/lib/python3.7/site-packages/networkx-2.2-py3.7.egg/networkx/classes/reportviews.py:95: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping, Set, Iterable\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import spikeextractors as se\n",
    "import spiketoolkit as st\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'detect_sign': -1, 'adjacency_radius': -1, 'freq_min': 300.0, 'freq_max': 6000.0, 'filter': False, 'curation': True, 'whiten': True, 'clip_size': 50, 'detect_threshold': 3, 'detect_interval': 10, 'noise_overlap_threshold': 0.15}\n",
      "Using 2 workers.\n",
      "Using tmpdir: /var/folders/s8/mkz4gjm57z3_4wqbv20t2jcm0000gn/T/tmp5l_5pf9k\n",
      "Num. workers = 2\n",
      "Preparing /var/folders/s8/mkz4gjm57z3_4wqbv20t2jcm0000gn/T/tmp5l_5pf9k/timeseries.hdf5...\n",
      "Preparing neighborhood sorters (M=7, N=600000)...\n",
      "Neighboorhood of channel 0 has 7 channels.\n",
      "Neighboorhood of channel 1 has 7 channels.\n",
      "Detecting events on channel 1 (phase1)...\n",
      "Detecting events on channel 2 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:00.152869\n",
      "Num events detected on channel 1 (phase1): 576\n",
      "Elapsed time for detect on neighborhood: 0:00:00.156341\n",
      "Num events detected on channel 2 (phase1): 685\n",
      "Computing PCA features for channel 1 (phase1)...\n",
      "Computing PCA features for channel 2 (phase1)...\n",
      "Clustering for channel 1 (phase1)...\n",
      "Found 1 clusters for channel 1 (phase1)...\n",
      "Computing templates for channel 1 (phase1)...\n",
      "Clustering for channel 2 (phase1)...\n",
      "Found 1 clusters for channel 2 (phase1)...\n",
      "Computing templates for channel 2 (phase1)...\n",
      "Re-assigning events for channel 1 (phase1)...\n",
      "Neighboorhood of channel 2 has 7 channels.\n",
      "Detecting events on channel 3 (phase1)...\n",
      "Re-assigning events for channel 2 (phase1)...\n",
      "Neighboorhood of channel 3 has 7 channels.\n",
      "Detecting events on channel 4 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:00.140797\n",
      "Num events detected on channel 3 (phase1): 753\n",
      "Computing PCA features for channel 3 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:00.126757\n",
      "Num events detected on channel 4 (phase1): 636\n",
      "Computing PCA features for channel 4 (phase1)...\n",
      "Clustering for channel 3 (phase1)...\n",
      "Clustering for channel 4 (phase1)...\n",
      "Found 1 clusters for channel 4 (phase1)...\n",
      "Computing templates for channel 4 (phase1)...\n",
      "Found 1 clusters for channel 3 (phase1)...\n",
      "Computing templates for channel 3 (phase1)...\n",
      "Re-assigning events for channel 3 (phase1)...\n",
      "Re-assigning events for channel 4 (phase1)...\n",
      "Neighboorhood of channel 4 has 7 channels.\n",
      "Neighboorhood of channel 5 has 7 channels.\n",
      "Detecting events on channel 5 (phase1)...\n",
      "Detecting events on channel 6 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:00.143806\n",
      "Num events detected on channel 5 (phase1): 658\n",
      "Elapsed time for detect on neighborhood: 0:00:00.141996\n",
      "Computing PCA features for channel 5 (phase1)...\n",
      "Num events detected on channel 6 (phase1): 623\n",
      "Computing PCA features for channel 6 (phase1)...\n",
      "Clustering for channel 6 (phase1)...\n",
      "Clustering for channel 5 (phase1)...\n",
      "Found 1 clusters for channel 6 (phase1)...\n",
      "Computing templates for channel 6 (phase1)...\n",
      "Found 1 clusters for channel 5 (phase1)...\n",
      "Computing templates for channel 5 (phase1)...\n",
      "Re-assigning events for channel 6 (phase1)...\n",
      "Neighboorhood of channel 6 has 7 channels.\n",
      "Detecting events on channel 7 (phase1)...\n",
      "Re-assigning events for channel 5 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:00.113207\n",
      "Num events detected on channel 7 (phase1): 657\n",
      "Computing PCA features for channel 7 (phase1)...\n",
      "Clustering for channel 7 (phase1)...\n",
      "Found 1 clusters for channel 7 (phase1)...\n",
      "Computing templates for channel 7 (phase1)...\n",
      "Re-assigning events for channel 7 (phase1)...\n",
      "Neighboorhood of channel 1 has 7 channels.\n",
      "Neighboorhood of channel 0 has 7 channels.\n",
      "Computing PCA features for channel 1 (phase2)...\n",
      "Computing PCA features for channel 2 (phase2)...\n",
      "No duplicate events found for channel 1 in phase2\n",
      "No duplicate events found for channel 0 in phase2\n",
      "Clustering for channel 1 (phase2)...\n",
      "Clustering for channel 2 (phase2)...\n",
      "Found 1 clusters for channel 1 (phase2)...\n",
      "Neighboorhood of channel 2 has 7 channels.\n",
      "Computing PCA features for channel 3 (phase2)...\n",
      "No duplicate events found for channel 2 in phase2\n",
      "Found 1 clusters for channel 2 (phase2)...\n",
      "Neighboorhood of channel 3 has 7 channels.\n",
      "Computing PCA features for channel 4 (phase2)...\n",
      "No duplicate events found for channel 3 in phase2\n",
      "Clustering for channel 4 (phase2)...\n",
      "Clustering for channel 3 (phase2)...\n",
      "Found 1 clusters for channel 4 (phase2)...\n",
      "Neighboorhood of channel 4 has 7 channels.\n",
      "Computing PCA features for channel 5 (phase2)...\n",
      "No duplicate events found for channel 4 in phase2\n",
      "Found 1 clusters for channel 3 (phase2)...\n",
      "Neighboorhood of channel 5 has 7 channels.\n",
      "Computing PCA features for channel 6 (phase2)...\n",
      "No duplicate events found for channel 5 in phase2\n",
      "Clustering for channel 5 (phase2)...\n",
      "Clustering for channel 6 (phase2)...\n",
      "Found 1 clusters for channel 5 (phase2)...\n",
      "Neighboorhood of channel 6 has 7 channels.\n",
      "Found 1 clusters for channel 6 (phase2)...\n",
      "Computing PCA features for channel 7 (phase2)...\n",
      "No duplicate events found for channel 6 in phase2\n",
      "Clustering for channel 7 (phase2)...\n",
      "Found 1 clusters for channel 7 (phase2)...\n",
      "Preparing output...\n",
      "Done with ms4alg.\n",
      "Cleaning tmpdir::::: /var/folders/s8/mkz4gjm57z3_4wqbv20t2jcm0000gn/T/tmp5l_5pf9k\n",
      "Curating\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import spikeextractors as se\n",
    "\n",
    "class SpikeElement:\n",
    "    \"\"\"Base class for spike elements\"\"\"\n",
    "\n",
    "    def __init__(self, interface_id, interface_class, interface_name):\n",
    "        self._interface_id = interface_id\n",
    "        self._interface_class = interface_class\n",
    "        self._interface_name = interface_name\n",
    "        self._params = copy.deepcopy(interface_class.gui_params())\n",
    "\n",
    "    @property\n",
    "    def interface_id(self):\n",
    "        return self._interface_id\n",
    "\n",
    "    @property\n",
    "    def interface_class(self):\n",
    "        return self._interface_class\n",
    "\n",
    "    @property\n",
    "    def interface_name(self):\n",
    "        return self._interface_name\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self._params\n",
    "\n",
    "    def setup(self):\n",
    "        pass\n",
    "\n",
    "    def run(self, input_payload=None):\n",
    "        pass\n",
    "\n",
    "    \n",
    "class Extractor(SpikeElement):\n",
    "    \"\"\"Extractor class\"\"\"\n",
    "\n",
    "    def __init__(self, interface_class, interface_id):\n",
    "        SpikeElement.__init__(self, interface_id, interface_class, interface_class.extractor_name)\n",
    "\n",
    "    def run(self, input_payload=None):\n",
    "        if(not self._interface_class.has_locations):\n",
    "            probe_path = self._params.pop(-1)['value']\n",
    "        params = self._params\n",
    "        params_dict = {}\n",
    "        for param in params:\n",
    "            param_name = param['name']\n",
    "            # param_type = param['type']\n",
    "            # param_title = param['title']\n",
    "            param_value = param['value']\n",
    "            params_dict[param_name] = param_value\n",
    "        recording = self._interface_class(**params_dict)\n",
    "        if(not self._interface_class.has_locations):\n",
    "            se.load_probe_file(recording, probe_path)\n",
    "        return recording\n",
    "    \n",
    "class Preprocessor(SpikeElement):\n",
    "    \"\"\"Preprocessor class\"\"\"\n",
    "\n",
    "    def __init__(self, interface_class, interface_id):\n",
    "        SpikeElement.__init__(self, interface_id, interface_class,\n",
    "                              interface_class.preprocessor_name)\n",
    "\n",
    "    def run(self, input_payload):\n",
    "        params_dict = {}\n",
    "        params_dict['recording'] = input_payload\n",
    "        params = self._params\n",
    "        for param in params:\n",
    "            param_name = param['name']\n",
    "            # param_type = param['type']\n",
    "            # param_title = param['title']\n",
    "            param_value = param['value']\n",
    "            params_dict[param_name] = param_value\n",
    "        pp = self._interface_class(**params_dict)\n",
    "        return pp\n",
    "\n",
    "class Sorter(SpikeElement):\n",
    "    \"\"\"Sorter class\"\"\"\n",
    "\n",
    "    def __init__(self, interface_class, interface_id):\n",
    "        SpikeElement.__init__(self, interface_id, interface_class,\n",
    "                              interface_class.sorter_name)\n",
    "\n",
    "    def run(self, input_payload):\n",
    "        base_sorter_param_dict = {}\n",
    "        base_sorter_param_dict['recording'] = input_payload\n",
    "            \n",
    "            \n",
    "        params = self._params\n",
    "        output_folder = params[0]\n",
    "        parallel = params[1]\n",
    "        base_sorter_param_dict[output_folder['name']] = output_folder['value']\n",
    "        base_sorter_param_dict[parallel['name']] = parallel['value']\n",
    "        sorter = self._interface_class(**base_sorter_param_dict)\n",
    "\n",
    "        sub_sorter_param_dict = {}\n",
    "        for param in params[2:]:\n",
    "            param_name = param['name']\n",
    "            # param_type = param['type']\n",
    "            # param_title = param['title']\n",
    "            param_value = param['value']\n",
    "            sub_sorter_param_dict[param_name] = param_value\n",
    "        sorter.set_params(**sub_sorter_param_dict)\n",
    "        sorter.run()\n",
    "\n",
    "        return sorter.get_result()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####EXTRACTOR######\n",
    "extractor_class = se.extractorlist.installed_recording_extractor_list[0]\n",
    "extractor = Extractor(extractor_class, 0)\n",
    "##User\n",
    "extractor.params[0]['value'] = 'sample_mountainsort_dataset'\n",
    "##\n",
    "payload = extractor.run()\n",
    "\n",
    "#####PREPROCESSOR######\n",
    "preprocessor_class = st.preprocessing.preprocessinglist.installed_preprocessers_list[0]\n",
    "preprocessor = Preprocessor(preprocessor_class, 1)\n",
    "payload = preprocessor.run(payload)\n",
    "\n",
    "######SORTER#######\n",
    "sorter_class = st.sorters.sorterlist.installed_sorter_list[0]\n",
    "sorter = Sorter(sorter_class, 1)\n",
    "payload = sorter.run(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighboorhood of channel 2 has 7 channels.\n",
      "Neighboorhood of channel 3 has 7 channels.\n",
      "Neighboorhood of channel 4 has 7 channels.\n",
      "Neighboorhood of channel 5 has 7 channels.\n",
      "Neighboorhood of channel 6 has 7 channels.\n"
     ]
    }
   ],
   "source": [
    "se.extractorlist.installed_recording_extractor_list[4].has_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properties of the in-memory dataset\n",
    "num_channels=7\n",
    "samplerate=30000\n",
    "duration=20\n",
    "num_timepoints=int(samplerate*duration)\n",
    "num_units=5\n",
    "num_events=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a pure-noise timeseries dataset and a linear geometry\n",
    "timeseries=np.random.normal(0,10,(num_channels,num_timepoints))\n",
    "geom=np.zeros((num_channels,2))\n",
    "geom[:,0]=range(num_channels)\n",
    "\n",
    "# Define the in-memory recording extractor\n",
    "RX=se.NumpyRecordingExtractor(timeseries=timeseries,geom=geom,samplerate=samplerate)\n",
    "\n",
    "# Generate some random events\n",
    "times=np.int_(np.sort(np.random.uniform(0,num_timepoints,num_events)))\n",
    "labels=np.random.randint(1,num_units+1,size=num_events)\n",
    "    \n",
    "# Define the in-memory sorting extractor\n",
    "SX=se.NumpySortingExtractor()\n",
    "for k in range(1,num_units+1):\n",
    "    times_k=times[np.where(labels==k)[0]]\n",
    "    SX.add_unit(unit_id=k,times=times_k)\n",
    "    \n",
    "#Add some features to the sorting extractor. These will be merged and split appropriately during curation\n",
    "spikes = 0\n",
    "for unit_id in SX.get_unit_ids():\n",
    "    SX.set_unit_spike_features(unit_id, feature_name='f_int', value=range(spikes, spikes + len(SX.get_unit_spike_train(unit_id))))\n",
    "    spikes += len(SX.get_unit_spike_train(unit_id))\n",
    "    \n",
    "spikes = 0\n",
    "for unit_id in SX.get_unit_ids():\n",
    "    SX.set_unit_spike_features(unit_id, feature_name='f_float', value=np.arange(float(spikes) + .1, float(spikes + len(SX.get_unit_spike_train(unit_id) + .1))))\n",
    "    spikes += len(SX.get_unit_spike_train(unit_id))\n",
    "    \n",
    "#Features that are not shared across ALL units will not be merged and split correctly (will disappear)\n",
    "SX.set_unit_spike_features(1, feature_name='bad_feature', value=np.repeat(1, len(SX.get_unit_spike_train(1))))\n",
    "SX.set_unit_spike_features(2, feature_name='bad_feature', value=np.repeat(2, len(SX.get_unit_spike_train(2))))\n",
    "SX.set_unit_spike_features(3, feature_name='bad_feature', value=np.repeat(3, len(SX.get_unit_spike_train(3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the API for extracting information\n",
    "print('Unit ids = {}'.format(SX.get_unit_ids()))\n",
    "st=SX.get_unit_spike_train(unit_id=1)\n",
    "print('Num. events for unit 1 = {}'.format(len(st)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can curate the results using a CuratedSortingExtractor\n",
    "CSX = se.CurationSortingExtractor(parent_sorting=SX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Curated Unit Ids: \" + str(CSX.get_unit_ids()))\n",
    "print(\"Original Unit Ids: \" + str(SX.get_unit_ids()))\n",
    "\n",
    "print(\"Curated ST: \" + str(CSX.get_unit_spike_train(1)))\n",
    "print(\"Original ST: \" + str(SX.get_unit_spike_train(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets split one unit from the sorting result (this could be two units incorrectly clustered as one)\n",
    "CSX.split_unit(unit_id=1, indices=[0, 1])\n",
    "print(\"Curated Unit Ids: \" + str(CSX.get_unit_ids()))\n",
    "print(\"Original Spike Train: \" + str(SX.get_unit_spike_train(1)))\n",
    "print(\"Split Spike Train 1: \" + str(CSX.get_unit_spike_train(6)))\n",
    "print(\"Split Spike Train 2: \" + str(CSX.get_unit_spike_train(7)))\n",
    "for unit_id in CSX.get_unit_ids():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the split was incorrect, we can always merge the two units back together\n",
    "CSX.merge_units(unit_ids=[6, 7])\n",
    "print(\"Curated Spike Train: \" + str(CSX.get_unit_spike_train(8)))\n",
    "print(\"Original Spike Train: \" + str(SX.get_unit_spike_train(1)))\n",
    "for unit_id in CSX.get_unit_ids():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also exclude units, so let's get rid of 8 since we are just confused about this unit\n",
    "CSX.exclude_units(unit_ids=[8])\n",
    "for unit_id in CSX.get_unit_ids():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's merge 3 and 4 together (This will create a new unit which encapsulates both previous units)\n",
    "CSX.merge_units(unit_ids=[3, 4])\n",
    "print(\"Curated Unit Ids: \" + str(CSX.get_unit_ids()))\n",
    "print(\"Merged Spike Train: \" + str(CSX.get_unit_spike_train(9)))\n",
    "print(\"Original Spike Trains concatenated: \" + str(np.sort(np.concatenate((SX.get_unit_spike_train(3), SX.get_unit_spike_train(4))))))\n",
    "print(\"\\nCuration Tree\")\n",
    "for unit_id in CSX.get_unit_ids():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's merge 2 and 6 together\n",
    "\n",
    "CSX.merge_units(unit_ids=[2, 9])\n",
    "print(\"Curated Unit Ids: \" + str(CSX.get_unit_ids()))\n",
    "print(\"Merged Spike Train: \" + str(CSX.get_unit_spike_train(10)))\n",
    "merged_spike_train = []\n",
    "for unit_id in SX.get_unit_ids():\n",
    "    if(unit_id != 1 and unit_id != 5):\n",
    "        merged_spike_train.append(SX.get_unit_spike_train(unit_id))\n",
    "merged_spike_train = np.asarray(merged_spike_train)\n",
    "merged_spike_train = np.sort(np.concatenate(merged_spike_train).ravel())\n",
    "print(\"Original Spike Trains concatenated: \" + str(merged_spike_train))\n",
    "print(\"\\nCuration Tree\")\n",
    "for unit_id in CSX.get_unit_ids():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's split unit 5 with given indices\n",
    "\n",
    "CSX.split_unit(unit_id=5, indices=[0, 1])\n",
    "print(\"Curated Unit Ids: \" + str(CSX.get_unit_ids()))\n",
    "print(\"Original Spike Train: \" + str(SX.get_unit_spike_train(5)))\n",
    "print(\"Split Spike Train 1: \" + str(CSX.get_unit_spike_train(11)))\n",
    "print(\"Split Spike Train 2: \" + str(CSX.get_unit_spike_train(12)))\n",
    "print(\"\\nCuration Tree\")\n",
    "for unit_id in CSX.get_unit_ids():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we can merge units 7 and 8\n",
    "\n",
    "CSX.merge_units(unit_ids=[10, 11])\n",
    "print(\"Curated Unit Ids: \" + str(CSX.get_unit_ids()))\n",
    "print(\"Merged Spike Train: \" + str(CSX.get_unit_spike_train(13)))\n",
    "original_spike_train = (np.sort(np.concatenate((SX.get_unit_spike_train(3), SX.get_unit_spike_train(4), SX.get_unit_spike_train(2), SX.get_unit_spike_train(5)[np.asarray([0,1])]))))\n",
    "print(\"Original Spike Train: \" + str(original_spike_train))\n",
    "print(\"\\nCuration Tree\")\n",
    "for unit_id in CSX.get_unit_ids():\n",
    "    CSX.printCurationTree(unit_id=unit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the input/output in the MountainSort format\n",
    "se.MdaRecordingExtractor.write_recording(recording=RX,save_path='sample_mountainsort_dataset')\n",
    "se.MdaSortingExtractor.write_sorting(sorting=CSX,save_path='sample_mountainsort_dataset/firings_true.mda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read this dataset with the Mda input extractor (we can now have a normal sorting extractor with our curations)\n",
    "RX2=se.MdaRecordingExtractor(dataset_directory='sample_mountainsort_dataset')\n",
    "SX2=se.MdaSortingExtractor(firings_file='sample_mountainsort_dataset/firings_true.mda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"New Unit Ids: \" + str(SX2.get_unit_ids()))\n",
    "print(\"New Unit Spike Train: \" + str(SX2.get_unit_spike_train(13)))\n",
    "print(\"Previous Curated Unit Spike Train: \" + str(CSX.get_unit_spike_train(13)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Current spike feature names\n",
    "print(CSX.get_unit_spike_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#All features have been appropriately merged and split according to previous operations\n",
    "print(CSX.get_unit_spike_features(12, 'f_int'))\n",
    "print(CSX.get_unit_spike_features(12, 'f_float'))\n",
    "print(CSX.get_unit_spike_features(13, 'f_int'))\n",
    "print(CSX.get_unit_spike_features(13, 'f_float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
